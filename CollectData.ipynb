{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fewer-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from Kafka import KafkaWriter, KafkaReader\n",
    "from OpenWeatherMap import OpenWeatherMap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-deployment",
   "metadata": {},
   "source": [
    "### Aufgabe 3 Buffern:\n",
    "-> using topic \"topic1\" for buffering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fleet-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the classes\n",
    "openWeatherMap = OpenWeatherMap()\n",
    "# defining a topic specific writer and reader for kafka \n",
    "writer1 = KafkaWriter('topic1')\n",
    "reader1 = KafkaReader('topic1')\n",
    "# defining a counter\n",
    "key = len(reader1.retrieve())\n",
    "\n",
    "def format_date(dt: str) -> str:\n",
    "    return datetime.fromtimestamp(float(dt)).strftime('%d.%m.%Y %H:%M')\n",
    "\n",
    "# load 'locations.json' into a json-object and return it\n",
    "def load_locations() -> json:\n",
    "    with open('locations.json', mode='r') as file:\n",
    "        return json.load(file)        \n",
    "\n",
    "# for each location, query openWeatherMap for the 5-day forecast and \n",
    "# store the the returned values in Kafka\n",
    "def collect_forecast_data(key) -> None:\n",
    "    # getting the locations from the json through the load_locations() method\n",
    "    locs = load_locations()\n",
    "    # going over the locations\n",
    "    for loc in locs:\n",
    "        # reading the data from the api\n",
    "        response = openWeatherMap.get_forecast(locs[loc])\n",
    "        # iterating over the elements in the given dict\n",
    "        for res in response['list']:\n",
    "            # adding a city and timestamp attribute to the dictionary\n",
    "            city = {'city' : response['city']['name'], 'coordinates':{'lat': locs[loc]['latitude'], 'lon': locs[loc]['longitude']}}\n",
    "            res.update(city)\n",
    "            # increasing the used counter\n",
    "            key = key + 1\n",
    "            # storing the data in kafka\n",
    "            writer1.store(str(key), res)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-eleven",
   "metadata": {},
   "source": [
    "### Aufgabe 4 Dubletten:\n",
    "-> using topic \"weather.forecast\" as \"clean\" topic with the eliminated duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "civil-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining kafka producer for the \"clean\" topic\n",
    "writer2 = KafkaWriter('weather.forecast')\n",
    "\n",
    "# method for eliminating the duplicates from the test-topic and storing the clean data in a new topic\n",
    "def cleaning() -> None:  \n",
    "    # getting all 'raw' data from the topic 'topic1'\n",
    "    all_data = reader1.retrieve()\n",
    "    \n",
    "    # getting the already exisiting 'filtered' data in 'weather.forecast'\n",
    "    existing_data = KafkaReader('weather.forecast').retrieve()\n",
    "    # defining a counter for the keys of the data in kafka\n",
    "    counter = len(existing_data)\n",
    "    \n",
    "    # looping through the 'raw' data, that means over every dictionary-element of it\n",
    "    for a in all_data:\n",
    "        # checking if existing_data is empty\n",
    "        if not existing_data:\n",
    "            # increase counter\n",
    "            counter = counter + 1\n",
    "            # if existing_data is empty store the first element of a in existing_data and append it to it for further checking\n",
    "            writer2.store(str(counter), a)\n",
    "            existing_data.append(a)\n",
    "        else:\n",
    "            # loop through the existing_data in the clean topic\n",
    "            for e in existing_data:\n",
    "                # check if a is already part of the clean topic\n",
    "                if a not in existing_data:\n",
    "                    # if it isn't already in the clean topic, increase counter, store it to kafka and append it to the list\n",
    "                    counter = counter + 1\n",
    "                    writer2.store(str(counter), a)\n",
    "                    existing_data.append(a)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "divided-start",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error sending OffsetCommitRequest_v2 to node coordinator-1 [Cancelled: <BrokerConnection node_id=coordinator-1 host=kafka-1:19092 <connected> [IPv4 ('172.18.0.5', 19092)]>]\n",
      "Marking the coordinator dead (node coordinator-1) for group 4d8a9a5c9086466fa5c4a64fc6e7ef97: Cancelled: <BrokerConnection node_id=coordinator-1 host=kafka-1:19092 <connected> [IPv4 ('172.18.0.5', 19092)]>.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-29f9b406abaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcollect_forecast_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#make half an hour / 15 min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-ec519c5480ad>\u001b[0m in \u001b[0;36mcleaning\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexisting_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexisting_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0mwriter2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# first reading in the data from the api with collect_forecast_data every 15 min\n",
    "# then reading the buffered data from 'topic1' and writing the unique values to the clean topic 'weather.forecast'\n",
    "while True:\n",
    "    collect_forecast_data(key)\n",
    "    cleaning()\n",
    "    time.sleep(900) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-ferry",
   "metadata": {},
   "source": [
    "<b>Idee des Ansatzes:</b> \\\n",
    "Da nicht ganz klar ist, wie oft die Wetterdaten von OpenWeatherMap aktualisiert werden, werden die Dictionary Elemente in beiden Topics miteinander verglichen. Dadurch ist gewährleistet, dass es keine Dubletten gibt. Jedoch wenn der Forecast in OpenWeatherMap aktualisiert wird und sich bspw. die Temperatur für eine Vorhersage ändert bzw. genauer wird, gilt dies nicht mehr als Dublette, sondern als aktualisierter Forecast und wird in Folge dessen auch in das cleane Topic geschrieben. Die doppelten Vorhersagen werden jedoch im Rahmen der get_dataframe() Methode vor der weiteren Verarbeitung herausgefiltert."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
